# ARC FOMAML-style pretraining config

defaults:
  - arch: trm
  - _self_

hydra:
  output_subdir: null

entity: "trelis"

# Data paths
data_paths: null
data_paths_test: null

load_checkpoint: null
checkpoint_path: null

evaluators:
  - name: arc@ARC

# Hyperparameters - Training
global_batch_size: 768

epochs: 25000
eval_interval: 2500
checkpoint_every_eval: True
checkpoint_every_n_steps: null
halt_max_steps_eval: null
puzzle_emb_reinit_strategy: "mean"

meta_learning:
  enabled: true
  inner_lr: null        # falls back to lr if left null
  inner_steps: 1

lr: 1e-4
lr_min_ratio: 1.0
lr_warmup_steps: 256

# Standard hyperparameter settings for LM, as used in Llama
beta1: 0.9
beta2: 0.95
weight_decay: 0.1
puzzle_emb_weight_decay: 0.1

# Hyperparams - Puzzle embeddings training
puzzle_emb_lr: 1e-2

seed: 0
min_eval_interval: 0

ema: True
ema_rate: 0.999
freeze_weights: True
freeze_weights_epochs: 256
