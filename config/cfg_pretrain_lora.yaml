# LoRA fine-tuning config

defaults:
  - arch: trm
  - _self_

hydra:
  output_subdir: null

entity: "trelis"

# Data path
data_paths: ['data/arc-manual-eval-aug-1000']
data_paths_test: ['data/arc-manual-eval-aug-1000']

load_checkpoint: null
checkpoint_path: null

arch:
  lora_rank: 1
  lora_alpha: 16.0
  lora_dropout: 0.0
  lora_train_base: false
  lora_train_bias: false

evaluators:
  - name: arc@ARC

# Hyperparams - Training
global_batch_size: 768

epochs: 12500
eval_interval: 250
checkpoint_every_eval: True
checkpoint_every_n_steps: null

lr: 1e-4
lr_min_ratio: 1.0
lr_warmup_steps: 2000

# Standard hyperparameter settings for LM, as used in Llama
beta1: 0.9
beta2: 0.95
weight_decay: 0.0
puzzle_emb_weight_decay: 0.1

# Hyperparams - Puzzle embeddings training
puzzle_emb_lr: 1e-2

seed: 0
min_eval_interval: 0 # when to start the eval

ema: False # disable Exponential-Moving-Average when running LoRA
ema_rate: 0.999 # unused when ema=False
freeze_weights: False # LoRA adapters handle freezing of linear weights; embeddings stay trainable
