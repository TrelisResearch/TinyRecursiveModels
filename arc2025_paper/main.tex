\documentclass[11pt,twocolumn]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{times}
\usepackage[numbers]{natbib}
\usepackage{booktabs}
\usepackage[hidelinks]{hyperref}
\usepackage{siunitx}

\title{Test-time Adaptation of Tiny Recursive Models}
\author{Ronan McGovern\\Trelis LTD\\\texttt{Trelis.com}}
\date{}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Tiny Recursive Models as a Basis for Solving ARC AGI II Tasks}

...

\subsection{Fitting within Compute Requirements for the ARC Prize 2025 Competition}

The ARC Prize 2025 competition allows for the use of four L4 accelerators for twelve hours. As a reference point, TRM pre-training on 1,120 tasks for 100k epochs takes roughly 48 hours on 4xH100 SXM GPUs. Accounting for a factor of roughly 8x between bf16 flops on a H100 SXM versus an L4 accelerator, there is only about 1/8 x 1/4 = 1/32th of the compute necessary to complete full pre-training on this model size. Cast in different terms, such pretraining takes approximately 750k optimizer steps, with a global batch size of 768, while the competition runtime allows for only about 15k optimizer steps at a batch size of 392, given that one must also allow time for inference/evaluation.

The premise of Test-time adaptation is that, by conducting pre-training prior to competition submissions, one can achieve better performance with fine-tuning than one could conducting pre-training from scratch.

\subsection{Approach and Contribution}
...

\section{Methods}\label{sec:methods}

A recursive transformer model was pre-trained on ARC AGI II training tasks in close accordance to the Tiny Recursive Model paper \cite{JolicoeurMartineau2025TinyRecursive}. During competition submissions, this pre-trained model was fully fine-tuned on the train example pairs of the test tasks. This fine-tuned model was used to predict test example outputs, using a majority voting method.

\subsection{Pre-training}
Three models were pre-trained. A first model model was trained almost exactly in line with the original TRM paper. A second model was pretrained with an expanded pre-training dataset and for double the original number of epochs. A third model was then pre-trained with a smaller dataset, filtered for tasks matching ARC AGI II public evaluation split difficulty.

\subsubsection{Original Paper Replication - 100k epochs}

A TRM was trained in close alignment with the original paper, but with only 4 lower reasoning cycles instead of 6 reported in the paper. This deviation was unintentional and resulted from a commit in the TRM Github repository using that same value. Interestingly, other ablations by Xin Gao (reference here) and Konstantin Schuerholt also use this value of 4.

The data mix matched that of the original paper and included:
\begin{itemize}
    \item ARC AGI II Training split [1000 tasks]: train + test example pairs
    \item Concept ARC split [160 tasks]: train + test example pairs
    \item ARC AGI II Evaluation split [120 tasks]: train example pairs only (test used for evaluation)
\end{itemize}

\subsubsection{Extended Data for 200k Epochs}

Following the heuristic that neural nets often improve in performance with longer pre-training, a second model was pre-trained for double the original number of epochs.

Following the heuristic of more higher in-distribution data helping the performance of neural nets, the dataset was expanded in two ways:
\begin{enumerate}
    \item Inclusion of evaluation split test example pairs: The test example pairs from 100 of the 120 public ARC AGI II evaluation split were included in pre-training. Of the remaining 20 tasks, ten were used as an evaluation split, and ten were used as a post-training test split. Concretely, train example pairs from 110 tasks were included in pre-training and test example pairs from 100 tasks were included in pre-training. Test example pairs from 10 tasks were withheld for evaluation during pre-training, while train AND test example pairs from 10 tasks were withheld entirely for use in post-training.
    \item Inclusion of 50 tasks from Simon Strandgaard's "tama" dataset \cite{StrandgaardTamaDataset}. These human-reviewed tasks cover concepts typical in ARC challenges and are of a difficulty somewhere between ARC AGI I and ARC AGI II. The hope for includiing this data was to broaden and enhance the pre-training dataset.
\end{enumerate}

In sum, this meant pre-training on:
\begin{itemize}
    \item ARC AGI II Training split [1000 tasks]: train + test example pairs
    \item Concept ARC split [160 tasks]: train + test example pairs
    \item ARC AGI II Evaluation split [110 tasks]: train example pairs from all 110 tasks and test example pairs from 100 tasks (the other ten serve as evaluation during pre-training)
    \item tama [50 tasks]: train + test example pairs
\end{itemize}

While there is the potential advantage of a higher quality, more in-distribution and larger pre-training dataset, there is a large trade-off in being able to assess performance with evaluation and test hold-out sets of only 10 tasks each, particularly when we are trying to assess performance with a granularity of one percent and where the anticipated score is in the region of 1-10 percent.

\subsubsection{Filtered Hard Data for 500k Epochs}

This third pre-trained variant aimed to test the heuristic that it can be better to train neural nets on a smaller amount of higher quality data for more epochs than on more mixed-quality data for fewer epochs.The same model and hyperparameters were used but training on 110 tasks from the ARC AGI II evaluation split and 120 hard tasks from the ARC AGI II training split. Training tasks were determined to be hard based on the ability of GPT-5-mini to write a python program that successfully solved all train and test example pairs. Specifically, any task for which GPT-5-mini could write a correct program, given approximately eight attempts, was filtered out. This left 137 remaining ARC AGI II training tasks, from which 120 were selected as a traininghard split. This filtering is somewhat arbitrary and skewed both because it filters based on a) python programming performance and b) LLM, not human, performance. The method was used because of prior unreported work attempting to solve ARC tasks by writing python programs. The choice of GPT-5-mini as a model was made because it was capable of solving only a few ARC AGI II evaluation tasks by writing python programs. As such, if a task can be solved through python program writing by GPT-5-mini this correlates with the task being too easy for ARC AGI II level tasks. And, the motivation for this "hard" dataset split was to have tasks more representative of the semi-private evaluation set on which performance is ultimately graded for the 2025 competition.

In sum, this meant pre-training on 230 tasks:
\begin{itemize}
    \item ARC AGI II Training split, filtered with GPT-5-mini program writing for "hard" tasks [120 tasks]: train + test example pairs
    \item ARC AGI II Evaluation split [110 tasks]: train example pairs from all 110 tasks and test example pairs from 100 tasks (the other ten serve as evaluation during pre-training)
\end{itemize}

\subsection{Post-training}
Four approaches to post-training were conducted:
\begin{enumerate}
    \item Full-fine tuning of a pretrained model.
    \item Fine-tuning of embeddings only.
    \item Full-fine tuning of a pretrained model, training only embeddings for the first quarter of optimizer steps.
    \item LoRA fine-tuning of a pretrained model.
\end{enumerate}

The fifth relevant approach -- and null hypothesis -- is to post-train on a randomly initialized model. For this baseline, we can look to results reported for the pre-training section above.

All post-training runs were conducted in the same manner and with the same hyperparameters as the pre-training runs, but with the following differences:
\begin{enumerate}
    \item At the start of each pre-training run, the model was initialised from a pretrained model in the previous section.
    \item Owing to lower total VRAM available on 4xL4s compared to 4xH100 SXMs, a global batch size of 392 was used instead of 768 for pre-training. Accordingly, the learning rate for the model trunk and for the embedding trunk were doubled (to 2e-4 and 2e-2, respectively).
    \item Owing to the compute limitation at competition time, training was run for only 15,000 optimizer steps, rather than $\sim$750k in the original pre-training.
    \item Post-training was conducted only on test tasks. No additional data was mixed in. For competition runs, this means running on semi-private test tasks, which can only be done by making a formal submission to the ARC Prize 2025 competition (noting that one submission is allowed daily).
    \item TRM uses a majority voting method to make test output predictions, defaulting to a vote over 1,000 augmented versions of each task. Since compute time is limited during the competition, only 256 or 512 augmentations are used at evaluation time -- although 1,000 are still used during pre- and post-training. The number of augmentations does not affect training time because epochs are defined as epochs over a given task and its variants (for a given task, a variant is sampled at random during each epoch).
\end{enumerate}

\subsection{Simulating and Measuring Post-training Performance}

Simulating post-training performance is difficult, as there are only 120 public evaluation tasks for ARC AGI II, and - if one is to post-train on those tasks - those same tasks cannot be included in one's pre-trained model. Three workarounds were attempted, with varying degrees of success.

\subsubsection{Post-training an ARC AGI I pre-trained model}
To assess the effectiveness of post training one can first pre-train a model on ARC AGI I tasks, i.e. the train and test example pairs from the 400 task training split, and the test pairs from the 400 task evaluation split. Such a model was already available on HuggingFace from Xin Gao \cite{Gao2025ARCAGI1TRM} and so that model was used without re-training.

This pre-trained model - which has only six tasks that also appear in the ARC AGI II public dataset [see notes on this in Appendix ...] - can then be post-trained on the train example pairs of ARC AGI II training and then used to make predictions of the corresponding test example pairs. Note that, for post-training to be effective, it is not a necessary condition that this approach show positive results, because ARC AGI I tasks are significantly easier than ARC AGI II tasks. As such, a model pre-trained with ARC AGI I may be too weak to provide a head start to post-training on ARC AGI II level tasks. As it turns out, the ARC AGI I pre-trained model IS strong enough to meaningfully help with post-training for ARC AGI II grade tasks.

\subsubsection{Post-training an ARC AGI II pre-trained model, but with 10 public eval tasks withheld as a test}
As an attempt at simulating post-training performance on an ARC AGI II pre-trained model, one can run a pretraining (as is described for the second, and third, models in the pre-training section) on ARC AGI II data but withholding 10 evaluation tasks as a test set for post-training.

This was done, but the signal from only ten test tasks is weak, noisy and not informative.

\subsubsection{Post-training an ARC AGI II pre-trained model via a formal ARC AGI 2025 Submission}
Given a positive result from pre-training an ARC AGI I model on ARC AGI II data, one can then take an ARC AGI II pretrained model (pretrained on data including ARC AGI II public eval training tasks) and submit that for post-training on the semi-private tasks in the competition. Of course, one cannot take an ARC AGI II pretrained model and easily simulate post-training because the train example pairs from evaluation tasks are already within the pre-training data. The assumption is that an ARC AGI II pre-trained model (i.e. trained on ARC AGI II public eval data) will be more in-distribution for the semi-private eval tasks than a model pre-trained only on what are much easier ARC AGI I tasks.

For the purpose of the competition, there were three pre-trained models submitted (the same three listed in the pre-training section):
1. The ARC AGI II pre-trained model. Here, post-training could not be simulated as it had been pre-trained on ARC AGI II evaluation train examples
2. An ARC AGI II pre-trained model (for 2x epochs, and with expanded data) - from which 10 tasks were withheld during pretraining. Here, post-training was simulated, but was too noisy to be informative given only 10 withheld tasks.
3. An ARC AGI II pre-trained model (for 500x epochs, and with a dataset filtered to include only 230 ARC AGI II difficulty tasks) - from which 10 tasks were withheld during pretraining. Again, post-training was simulated but too noisy to be informative.

However, since all three pre-trained model were submitted to the competition for pre-trainning, semi-private results ARE reported.

\subsection{Post-training Methods}
Four post-training methods are considered and evaluated on an ARC AGI I pre-trained model using ARC AGI II tasks. Based on those results, the chosen approach for competition submissions is to fine-tune embeddings for one quarter of post-training epochs and then fully fine-tuning for the remaining epochs.

\subsubsection{Full fine-tuning}
Full fine-tuning involved updating all model parameters, as in pre-training. The only differences were those listed above.

\subsubsection{Fine-tuning of embeddings only.}
In TRM, each task and any augmented variant of that task is assigned a task id and has it's own trainable embedding. At the start of post-training, the model sees a new set of tasks and variants, and so a new set of task ids and set of embeddings are initialised. These embeddings start the post-training process untrained, while the rest of the model parameters (attention, MLPs and heads) start in a pre-trained state.

As such, at the start of post-training, the task embeddings - which might be thought of as a description of the transformation involved in a given task variant - are out of sync with the model's trunk. With this as motiviation, it appears reasonable to train only the embeddings to see whether they might be adapted by post-training in a manner that allows the model trunk to "execute" a program described by a newly trained task id. Were the training of embeddings sufficient, this might suggest that the trunk has the required tools to solve ARC tasks and the embedding need only be trained to signal to the trunk what tools should be used for the task at hand.

\subsubsection{Fine-tuning of embeddings and then full-finetuning}
As it turns out, post-training only of the embeddings results in a score of near zero on held out tasks. This motivates the idea of first training only the embeddings for some epochs (one quarter of total post-training epochs) followed by full fine-tuning for the remainder of epochs to allow for full fine-tuning.

\subsubsection{LoRA fine-tuning of the trunk plus embeddings fine-tuning}
Rather than train all parameters in the trunk (everything but the embeddings) one might instead train a set of low rank adapters for linear layers in the trunk. This reduces the number of parameters that are trainable. The motivation is potentially to avoid overfitting by training all parameters. However, TRMs are already low in parameter count, and the use of LoRA may reduce the ability of the model to adapt to the newly presented tasks. This appears to be the case.


\section{Results}

\subsection{Pre-Training}

All results for the TRM Paper ARC AGI II Replication Run are available at \url{https://wandb.ai/trelis/Arc2concept-aug-1000-ACT-torch},  see \texttt{pretrain\_att\_arc2concept\_4}, and the associated pre-trained model checkpoint is released at \cite{Trelis2025TRMARCAGII}.

Results for the extended training runs are available at \url{https://wandb.ai/trelis/Arc2-pretrain-final-ACT-torch}, and the released checkpoints are \cite{Trelis2025TRMARCAGIIAll200k} and \cite{Trelis2025TRMARCAGIIHard1M}.

Select results are shown here from the replication rather than the extended runs, because the evaluation set of ten tasks in the extended runs is too small to provide statistically meaningful insights on performance.

Figure \ref{fig:pass_accuracy} shows the evolution of pass@2 and pass@1000 accuracy on the 120 ARC AGI II public eval tasks during the training run, reaching a final pass@2 score of ~10\%, which is higher than the originally reported score of 7.8\% in the TRM paper, presumably associated with stochastic behaviour during training. Figure \ref{fig:exact_accuracy} shows the per-example-pair exact accuracy (i.e. percentage of training pairs the model gets correct within a training batch of 768 example pairs) over the course of training. It is clear that the model heavily overfits the data, although continued training gradually brings up the exact accuracy on the evaluation set, although still far below 100\% by the end of training.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/pass_accuracy.pdf}
    \caption{ARC evaluation pass@2 and pass@1000 accuracies throughout training for the \texttt{pretrain\_att\_arc2concept\_4} replication run.}
    \label{fig:pass_accuracy}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/exact_accuracy.pdf}
    \caption{Evaluation and train exact accuracies throughout training for the \texttt{pretrain\_att\_arc2concept\_4} replication run.}
    \label{fig:exact_accuracy}
\end{figure}

\subsection{Post-Training}
\subsubsection{Post-training of the ARC AGI I Pre-trained Model}
Post-training sweeps were conducted on the ARC AGI I checkpoint released by Xin Gao \cite{Gao2025ARCAGI1TRM}. All experiments were logged in the \texttt{Arc-eval2-aug-1000-ACT-torch} Weights \& Biases project \cite{Trelis2025PosttrainEval2}. Figure~\ref{fig:posttrain_pass_accuracy} compares evaluation pass@2 during the first $6$k optimisation steps for five adaptation strategies: full fine-tuning (\texttt{posttrain\_aa1\_aa2e}), embeddings-only (\texttt{posttrain\_aa1\_aa2e\_fe}), embeddings-only followed by full fine-tuning after a quarter (\texttt{posttrain\_aa1\_aa2e\_feq}) or half (\texttt{posttrain\_aa1\_aa2e\_feh}) of the steps, and LoRA (\texttt{posttrain\_aa1\_aa2e\_lora}).

The highest gains in accuracy are achieved by full fine-tuning OR by training only embeddings for a first portion of epochs (either half or one quarter), followed by full-finetunig for the rest. Fine-tuning embeddings only OR LoRA fine-tuning (which additionally includes fine-tuning of embeddings) lead to low accuracy. Note that there is a signicant level of noise involved in these results. For example, LoRA+embeddings fine-tuning looks inferior to fine-tuning embeddings only in the pass@2 metrics. However, looking at a broader set of results - available in Weights and Biases - reveals that LoRA+embeddings slightly outperforms embeddings-only when looking at pass@1000, a somewhat less noisy indicator of performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/posttrain_pass_accuracy.pdf}
    \caption{ARC evaluation pass@2 for the ARC AGI I post-training sweeps logged in \cite{Trelis2025PosttrainEval2}. Each curve shows the first $6$k optimisation steps for a different adaptation strategy.}
    \label{fig:posttrain_pass_accuracy}
\end{figure}

Note that while only 6k optimizer steps were used for the above sweeps, competition submissions used either 12,000 or 15,000 optimizer steps.

\subsubsection{Post-training of ARC AGI II pre-trained models during competition submissions}
Simulating ARC AGI II checkpoints provides limited insight: the public evaluation set is already included in pre-training and the ten-task hold-out used for diagnostics offers too small a sample to estimate performance reliably. For completeness these diagnostics, recorded at \cite{Trelis2025PosttrainEval2Holdout}, achieve zero pass@2 after post-training across all models.

Three pre-trained models were nevertheless submitted to the ARC Prize 2025 runtime for competition post-training. All submissions reused the configuration in Section~\ref{sec:methods}, with the baseline TRM replication fine-tuned for $12.5$k steps and the extended variants for $15$k steps to stay within the twelve-hour budget. Table~\ref{tab:semi_private_scores} summarises the resulting semi-private evaluation scores. Re-submitting the same model often yielded scores ranging from $3.33$ to $6.67$ depending on minor procedural tweaks (for example, adjusting the duration of frozen trunk updates or adding brief continued pre-training), underscoring the high variance inherent in the evaluation process.

\begin{table}[t]
    \centering
    \caption{Semi-private evaluation accuracy achieved after post-training ARC AGI II checkpoints within the competition environment.}
    \label{tab:semi_private_scores}
    \begin{tabular}{@{}lS[table-format=1.2]@{}}
        \toprule
        Pre-trained model & {Accuracy (\%)} \\
        \midrule
        TRM paper replication & 6.67 \\
        Expanded data, 200k epochs & 4.25 \\
        Filtered hard data, 1M epochs & 1.27 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{Discussion}

\subsection{Pre-training is required for limited compute competition environments}
...

\subsection{On the use of augmentation-specific task embeddings}

Both HRM and TRM assign a unique embedding not just to each task, but to every augmented variant (flips, rotations, re-colours) of each task. From the model's perspective, each task variant is an entirely separate task. The model may or may not learn that these tasks are related.

It is interesting to ask whether:
\begin{enumerate}
    \item the model does learn that variants of the same task are related,
    \item it would be more efficient to use the same task\_id/embedding for all variants of a given task OR whether treating variants as independent tasks provides the model with some useful form of regularisation/generality during pre-training.
\end{enumerate}

As a means of partially addressing (1), one might look at the cosine of the angle between task embeddings for i) variants of the same task and ii) between the base/original/unvaried form of different tasks. If the TRM does indeed learn to encode task variants similarly, perhaps one should expect a rise in the cosine between embeddings during pre- and post-training. With this as motivation, these measures were recorded during the pretraining of the TRM for 200k epochs on the extended dataset (extended to include 'tama' and ARC EVAL II evaluation test examples, see \ref{sec:methods}).

Figure \ref{fig:embedding_cosine_within} shows the evolution of the cosine of the angle between task embeddings of variants of the same task (averaged across tasks in a given batch), for training and evaluation sets. Note that the angle between task id embeddings is reported as zero at most steps because there is at most one variant of each task per batch of 768 example pairs. If one looks in greater granularity at the data, there are timesteps reaching cosine similarity of up to about 0.1 . This is more apparent in the training of the third model on the subset of hard tasks (not shown in these plots), since there are only 230 base task ids and every batch therefore includes more than one variant for each task. Nonetheless, the cosine similarity of task id embeddings for variants of the same task is low when measured on the training set. The cosine similarity is higher, and rising throughout training, when measured on the evaluation set. It is not obvious why training and evaluation sets diverge here and why the model might adapt to develop more similarity on evaluation example pairs. 

Figure \ref{fig:embedding_cosine_across} illustrates the evolution - over the course of training - of the cosine of the angle between the base variants of different tasks within the same training batch. Although there is some gap between the cosine on training and evaluation examples, both rise together, although the measurement seems to asymptote more on the training examples. Interestingly, comparing \ref{fig:embedding_cosine_across} and \ref{fig:embedding_cosine_within}, task id embeddings appear similarly distant within tasks as between tasks, suggesting the relationship between variants of the same task are not clearly expressed - at least within the embeddings alone.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/embedding_cosine_within.pdf}
    \caption{Embedding cosine similarity among augmented variants of the same task during extended pre-training.}
    \label{fig:embedding_cosine_within}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/embedding_cosine_across.pdf}
    \caption{Embedding cosine similarity across base tasks for the extended pre-training runs.}
    \label{fig:embedding_cosine_across}
\end{figure}

\subsection{On the benefit of joint training on multiiple tasks for compute efficiency}

If post training requires fewer optimizer steps than training from scratch -- to reach the same performance -- then clearly there is meaningful inter-task learning OR at least there are shared concepts involved in training TRM.

The effect can also be understood by pretraining on a single task versus 8 versus 120 tasks on a model of the same size. While it appears possible to achieve similar performance when training on a task individually, or combined with other tasks, it is more efficient -- again for a model of fixed size -- to jointly train on multiple tasks. Said differently, one can reach the same performance with less compute if one jointly trains on multiple tasks. As such, there appears to be joint concepts to be learned. It is possible of course that much of this joint learning involves primitive concepts relating to grid sizes and colours, as opposed to transformations themselves.

\subsection{Tiny Recursive Models as a Variant of Searching Latent Program Space (SLPS)}

...

\subsection{On the distribution of ARC AGI II tasks}

A major challenge in ARC AGI II is that there is little public data that is clearly in the distribution of the eventual ARC AGI II semi-private dataset. It is known that any approach scores remarkably similarly on the ARC AGI II public evaluation set and on the ARC AGI II semi-private (and likely private?) dataset. This means that those datasets are closely in-distribution. By contrast, the ARC AGI II training dataset (and the hard split) appears not to be in distribution as scores do not correlate closely with ARC AGI II evaluation sets.

For the purpose of research, it would have been highly useful to have an ARC AGI II training split of 120 tasks in the same distribution as the public eval and semi-private eval set. This would have allowed for pre-training on such a set, followed by post-training on the public eval set to accurately assess performance.

The closest approximation of this would perhaps have been to pre-train on 60 of the 120 public eval tasks, and post-train on the other 60. However, this has two drawbacks:
\begin{itemize}
    \item Statistical power, already small at just 120 tasks, is even smaller when one takes a subsplit.
    \item Model performance is sensitive to the amount of data that must be encoded. For the same model size, one cannot directly compare the performance pre or post training on 120 tasks versus 60 tasks.
\end{itemize}

\section{Ablations}
\subsection{On the method of embedding initialisation in post-training}

\subsection{On improvements to majority voting}

\section{Conclusion and future work}

Full fine-tuning allowed a pre-trained tiny recursive transformer model to be efficiently adapted in the compute-limited environment of the ARC AGI II competition. Effective adaptation appears to require updating both task id embeddings AND the trunk of the model at competition time.

Currently, the use of tiny recursive models has been limited to single or low double digit scores on ARC AGI II tasks. While post-training of a pretrained TRM is much more compute efficient than pre-training from scratch, it has not been shown that post-training can exceed or reach the performance achieved in pre-training. That pass@1000 metrics reach well above pass@2 metrics, often above 20\% on ARC AGI II difficulty tasks, suggests that performance improvements are possible. It remains an open question how far performance could be pushed through model size or hyper parameter improvements with or without further data additions or augmentations.

\section{Acknowledgements}

This work was supported by Runpod, which provided \$15k of compute, and by Lambda Labs, which provided \$1k of compute. Thanks to Lewis Hemens for months of collaboration on ARC Prize research and support for compute costs. Thanks to Jack Boylan for assistance in running the pre-training replication.

\bibliographystyle{plainnat}
\bibliography{references}

\clearpage
\onecolumn
\appendix
\section{ARC Task Example Data Splits}

\begin{table}[t]
    \centering
    \small
    \caption{Raw dataset splits used across experiments.}
    \label{tab:raw_splits}
    \begin{tabular}{lrrrr}
        \toprule
        \textbf{Challenges file} & \textbf{Puzzles} & \textbf{Avg. train inputs} & \textbf{Avg. test inputs} \\
        \midrule
        \texttt{arc-agi\_concept\_challenges.json} & 160 & 2.67 & 3.00 \\
        \texttt{arc-agi\_training2\_challenges.json} & 1000 & 3.23 & 1.08 \\
        \texttt{arc-agi\_evaluation2\_challenges.json} & 120 & 2.98 & 1.43 \\
        \texttt{arc-agi\_tama\_challenges.json} & 50 & 3.18 & 1.52 \\
        \texttt{arc-agi\_test\_challenges.json} & 240 & 3.20 & 1.08 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[t]
    \centering
    \small
    \caption{Derived splits constructed for extended pre-training and evaluation.}
    \label{tab:derived_splits}
    \begin{tabular}{lrrrr}
        \toprule
        \textbf{Challenges file} & \textbf{Puzzles} & \textbf{Avg. train inputs} & \textbf{Avg. test inputs} \\
        \midrule
        \texttt{arc-agi\_evaluation2train\_challenges.json} & 100 & 2.96 & 1.44 \\
        \texttt{arc-agi\_evaluation2eval\_challenges.json} & 10 & 2.90 & 1.60 \\
        \texttt{arc-agi\_evaluation2test\_challenges.json} & 10 & 3.30 & 1.20 \\
        \texttt{arc-agi\_traininghard\_challenges.json} & 120 & 2.98 & 1.09 \\
        \texttt{arc-agi\_evaluation2clean\_challenges.json} & 114 & 2.97 & 1.46 \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Notes on derived splits.} The \texttt{evaluation2train}, \texttt{evaluation2eval}, and \texttt{evaluation2test} files are all sampled from the ARC AGI II evaluation split. These subsets supply the pre-training and post-training tasks for the second model configuration.

\paragraph{Side notes.}
\begin{itemize}
    \item Six tasks in the ARC AGI II evaluation split also appear in ARC AGI I. When adapting models pre-trained on ARC AGI I, the \texttt{arc-agi\_evaluation2clean\_challenges.json} split filters these duplicates to avoid contamination.
    \item The placeholder \texttt{arc-agi\_test\_challenges.json} split contains fewer test examples than the evaluation set. Inference on this set is roughly 33\% faster than the final competition rerun and can under-estimate runtime, risking notebook timeouts during submission.
\end{itemize}
\end{document}
